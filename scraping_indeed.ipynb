{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Indeed Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Here we scrape the HTML data from the <a href=\"https://www.indeed.com/\">Indeed</a> jobs web page.\n",
    "\n",
    "It is used the Python 'requests' library to read the HTML content of the page, Beautifulsoup module to convert the \n",
    "content to a readable format and Pandas for data manipulation.\n",
    "\n",
    "The job information is extracted using different search criteria and the data is saved to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the Indeed webpage involved mainly the following steps:\n",
    "- access the HTML content from the webpage\n",
    "- search and extract the job details\n",
    "- save the results to a csv file\n",
    "- repeat the steps above using different search criteria\n",
    "- merge all data into a final csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define functions for web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function **extract()** accesses the HTML content from the webpage using requests and Beautiful Soup libraries. It uses the page \n",
    "URL that includes the search criteria under 'q' (query) and 'l' (location), e.g. 'data scientist', 'Washington State'.\n",
    "\n",
    "The second function **transform()** is used for searching and extracting all the details for a job posting, saves them to a dictionary, then \n",
    "appends all dictionaries to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(page):\n",
    "    \"\"\"Function to extract HTML content into Python script\"\"\"\n",
    "    \n",
    "    # URL of the Indeed page\n",
    "    # (e.g. search criteria 'data scientist', 'Washington State')\n",
    "    url = f\"https://www.indeed.com/jobs?q=data%20scientist&l=Washington%20State&start={page}\"                  \n",
    "    \n",
    "    # Read the content of the HTML page\n",
    "    r = requests.get(url)   \n",
    "    \n",
    "    # Convert the HTML content to a readable \n",
    "    # format using BeautifulSoup\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(soup):\n",
    "    \"\"\"Function to extract all job details for a posting in a\n",
    "       dictionary, then append all dictionaries to a list.\"\"\"\n",
    "    \n",
    "    # use find_all() function to view the info inside \"div\" container\n",
    "    divs = soup.find_all(\"div\", class_=\"job_seen_beacon\")\n",
    "    \n",
    "    # Iterate through each div container and get job details\n",
    "    for item in divs:\n",
    "        # title\n",
    "        job_title = item.h2.find_all(\"span\")\n",
    "        if len(job_title) == 2:\n",
    "            title = item.h2.find_all(\"span\")[1].text.strip()\n",
    "        else:\n",
    "            title = item.h2.find_all(\"span\")[0].text.strip()\n",
    "        # company\n",
    "        company = item.find(\"span\", class_=\"companyName\").text.strip()\n",
    "        # location\n",
    "        location = item.find(\"div\", class_=\"companyLocation\").text.strip()\n",
    "        # salary\n",
    "        try:\n",
    "            salary = item.find(\"div\", {\"class\": \"metadata salary-snippet-container\"}).text.strip()\n",
    "        except:\n",
    "            salary = ''\n",
    "        # summary\n",
    "        summary = item.find(\"div\", {\"class\": \"job-snippet\"}).text.strip().replace('\\n', '')\n",
    "\n",
    "        # Create a dictionary with all job details\n",
    "        job = {\n",
    "            'Title': title,\n",
    "            'Company': company,\n",
    "            'Location': location,\n",
    "            'Salary': salary,\n",
    "            'Summary': summary           \n",
    "        }\n",
    "        \n",
    "        # Append all dictionaries to a list\n",
    "        joblist.append(job)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scrape multiple webpages using specific search criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Extract job details from multiple pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page, 40\n",
      "Getting page, 50\n",
      "Getting page, 60\n",
      "Getting page, 70\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# Search criteria: 'data scientist', 'Washington State'\n",
    "\n",
    "# Create a list of dictionaries\n",
    "# Each dictionary contains details about one job\n",
    "joblist = []\n",
    "\n",
    "# Extract jobs details for 4 web pages\n",
    "# e.g. pages 40 to 70\n",
    "for i in range(40, 80, 10):           \n",
    "    print(f'Getting page, {i}')\n",
    "    c = extract(0)\n",
    "    transform(c)\n",
    "\n",
    "# print the number of jobs found in these pages\n",
    "print(len(joblist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Save the scraped data to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the 'joblist' with 60 jobs found\n",
    "# The list of jobs correspond to search criteria 'data scientist', 'Washington State'\n",
    "df = pd.DataFrame(joblist)\n",
    "\n",
    "# Print shape and first rows\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a csv file\n",
    "df.to_csv('jobsDS.csv', sep =\",\", index=False)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Repeat steps for scraping data using different search criteria. Save data into a final dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different search criteria that were used include 'ML', 'azure', 'Tableau', 'entry level' etc.. The data was saved to\n",
    "csv files, named according to the criteria used.\n",
    "\n",
    "At the end, we merge all data into a final csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1296, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load data from all job files into dataframes\n",
    "# Merge the dataframes into a final one with all data\n",
    "\n",
    "jobs1 = pd.read_csv(\"jobsAI1.csv\")\n",
    "jobs2 = pd.read_csv(\"jobsAI2.csv\")\n",
    "jobs3 = pd.read_csv(\"jobsAWS1.csv\")\n",
    "jobs4 = pd.read_csv(\"jobsAWS2.csv\")\n",
    "jobs5 = pd.read_csv(\"jobsAzure1.csv\")\n",
    "jobs6 = pd.read_csv(\"jobsAzure2.csv\")\n",
    "jobs7 = pd.read_csv(\"jobsDataEng1.csv\")\n",
    "jobs8 = pd.read_csv(\"jobsDataEng2.csv\")\n",
    "jobs9 = pd.read_csv(\"jobsDeepLearn1.csv\")\n",
    "jobs10 = pd.read_csv(\"jobsDeepLearn2.csv\")\n",
    "jobs11 = pd.read_csv(\"jobsDS1.csv\")\n",
    "jobs12 = pd.read_csv(\"jobsDS2.csv\")\n",
    "jobs13 = pd.read_csv(\"jobsDSEntryLevel.csv\")\n",
    "jobs14 = pd.read_csv(\"jobsDSIntern.csv\")\n",
    "jobs15 = pd.read_csv(\"jobsDSSenior1.csv\")\n",
    "jobs16 = pd.read_csv(\"jobsDSSenior2.csv\")\n",
    "jobs17 = pd.read_csv(\"jobsML1.csv\")\n",
    "jobs18 = pd.read_csv(\"jobsML2.csv\")\n",
    "jobs19 = pd.read_csv(\"jobsPowerBI1.csv\")\n",
    "jobs20 = pd.read_csv(\"jobsPowerBI2.csv\")\n",
    "jobs21 = pd.read_csv(\"jobsTableau1.csv\")\n",
    "jobs22 = pd.read_csv(\"jobsTableau2.csv\")\n",
    "\n",
    "all_jobs = pd.concat([jobs1, jobs2, jobs3, jobs4, jobs5, jobs6, jobs7, jobs8, jobs9, jobs10,\n",
    "               jobs11, jobs12, jobs12a, jobs13, jobs14, jobs15, jobs16, jobs17, jobs18, jobs19, jobs20,\n",
    "               jobs21, jobs22], axis=0)\n",
    "print(all_jobs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a csv file 'indeed_jobs.csv'\n",
    "all_jobs.to_csv(\"indeed_jobs.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check the final dataframe with all scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe with all jobs:  (1296, 5)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI/ML - Chief of Staff, Machine Intelligence</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Machine Intelligence team is accelerating ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist, Rich Media Experiences</td>\n",
       "      <td>Zillow</td>\n",
       "      <td>Washington State•Remote</td>\n",
       "      <td>$127,100 - $203,000 a year</td>\n",
       "      <td>Using Computer Vision techniques and AI-powere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>Zillow</td>\n",
       "      <td>Washington State•Remote</td>\n",
       "      <td>$132,400 - $211,600 a year</td>\n",
       "      <td>These algorithms and platforms ingest large vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Zillow</td>\n",
       "      <td>Washington State•Remote</td>\n",
       "      <td>$127,100 - $203,000 a year</td>\n",
       "      <td>This data can be mined for intelligence on com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Research Program Manager, Artificial Intelligence</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We drive efficiency, cultivate relationships, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title   Company  \\\n",
       "0       AI/ML - Chief of Staff, Machine Intelligence     Apple   \n",
       "1      Senior Data Scientist, Rich Media Experiences    Zillow   \n",
       "2                          Machine Learning Engineer    Zillow   \n",
       "3                              Senior Data Scientist    Zillow   \n",
       "4  Research Program Manager, Artificial Intelligence  Facebook   \n",
       "\n",
       "                  Location                      Salary  \\\n",
       "0              Seattle, WA                         NaN   \n",
       "1  Washington State•Remote  $127,100 - $203,000 a year   \n",
       "2  Washington State•Remote  $132,400 - $211,600 a year   \n",
       "3  Washington State•Remote  $127,100 - $203,000 a year   \n",
       "4             New York, NY                         NaN   \n",
       "\n",
       "                                             Summary  \n",
       "0  The Machine Intelligence team is accelerating ...  \n",
       "1  Using Computer Vision techniques and AI-powere...  \n",
       "2  These algorithms and platforms ingest large vo...  \n",
       "3  This data can be mined for intelligence on com...  \n",
       "4  We drive efficiency, cultivate relationships, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the csv file with all scraped data\n",
    "jobs = pd.read_csv(\"indeed_jobs.csv\")\n",
    "\n",
    "# Print the shape and the first rows\n",
    "print(\"Shape of dataframe with all jobs: \", jobs.shape)\n",
    "print()\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The final dataframe has 1296 rows and 5 columns**. Each row contains **details** about one job posting: **Title, Company, \n",
    "Location, Salary and Summary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
